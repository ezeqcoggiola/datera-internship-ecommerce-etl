{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c0c0d",
   "metadata": {},
   "source": [
    "# **Proyecto Final - PasantÃ­a de IngenierÃ­a de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd537",
   "metadata": {},
   "source": [
    "Para generar las credenciales ejecuto este comando en terminal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws sts get-session-token --duration-seconds 14400 --output json | Out-File -FilePath \"$env:USERPROFILE\\aws-temp-creds.json\" -Encoding utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e62096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AIDAVNFKRAMYBQN25WWQO', 'Account': '371872301872', 'Arn': 'arn:aws:iam::371872301872:user/ezequiel.coggiola', 'ResponseMetadata': {'RequestId': 'f50d7f03-b0ea-4a2f-b8af-060752216009', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f50d7f03-b0ea-4a2f-b8af-060752216009', 'x-amz-sts-extended-request-id': 'MTp1cy1lYXN0LTE6UzoxNzYzNjQ2NDc2MzM0OlI6VndGUExzZFk=', 'content-type': 'text/xml', 'content-length': '414', 'date': 'Thu, 20 Nov 2025 13:47:56 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# credenciales\n",
    "path = Path.home() / \"aws-temp-creds.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    creds = json.load(f)[\"Credentials\"]\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = creds[\"AccessKeyId\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = creds[\"SecretAccessKey\"]\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = creds[\"SessionToken\"]\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "\n",
    "sts = boto3.client(\"sts\")\n",
    "print(sts.get_caller_identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c41da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conexiÃ³n a s3\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "dataset_name = \"ecommerce\"\n",
    "bucket_name = f\"{dataset_name}-ezequiel-2025\"\n",
    "folders = [\"raw/\", \"processed/\", \"curated/\"]\n",
    "\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "\n",
    "s3 = boto_sess.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47823b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bucket 'ecommerce-ezequiel-2025' creado en us-east-1\n",
      "   - Creada estructura: raw/\n",
      "   - Creada estructura: processed/\n",
      "   - Creada estructura: curated/\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear el bucket (en us-east-1 no se usa LocationConstraint)\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "print(f\"âœ… Bucket '{bucket_name}' creado en {region}\")\n",
    "\n",
    "# 2. Activar versioning\n",
    "s3.put_bucket_versioning(\n",
    "    Bucket=bucket_name,\n",
    "    VersioningConfiguration={\"Status\": \"Enabled\"}\n",
    ")\n",
    "\n",
    "# 3. Configurar default encryption (SSE-S3)\n",
    "s3.put_bucket_encryption(\n",
    "    Bucket=bucket_name,\n",
    "    ServerSideEncryptionConfiguration={\n",
    "        \"Rules\": [\n",
    "            {\n",
    "                \"ApplyServerSideEncryptionByDefault\": {\n",
    "                    \"SSEAlgorithm\": \"AES256\"  # SSE-S3\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. Block Public Access\n",
    "s3.put_public_access_block(\n",
    "    Bucket=bucket_name,\n",
    "    PublicAccessBlockConfiguration={\n",
    "        \"BlockPublicAcls\": True,\n",
    "        \"IgnorePublicAcls\": True,\n",
    "        \"BlockPublicPolicy\": True,\n",
    "        \"RestrictPublicBuckets\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5. Agregar tags al bucket\n",
    "s3.put_bucket_tagging(\n",
    "    Bucket=bucket_name,\n",
    "    Tagging={\n",
    "        \"TagSet\": [\n",
    "            {\"Key\": \"owner\", \"Value\": \"Ezequiel Coggiola\"},\n",
    "            {\"Key\": \"team\", \"Value\": \"Datera\"},\n",
    "            {\"Key\": \"env\", \"Value\": \"dev\"},\n",
    "            {\"Key\": \"dataset\", \"Value\": dataset_name}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# 6. Crear la estructura de \"carpetas\" (raw/, processed/, curated/)\n",
    "for folder in folders:\n",
    "    s3.put_object(Bucket=bucket_name, Key=folder)\n",
    "    print(f\"   - Creada estructura: {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "158db537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "dataset_name = \"ecommerce_ezequiel\"   # o pasalo como parÃ¡metro\n",
    "\n",
    "def upload_files(local_folder, bucket_name, layer=\"raw\", file_types=None):\n",
    "    \"\"\"\n",
    "    Sube archivos a S3 creando automÃ¡ticamente una carpeta por dataset.\n",
    "    Ejemplo:\n",
    "        raw/customers/customers.csv\n",
    "        raw/orders/orders.csv\n",
    "    \"\"\"\n",
    "    if file_types is None:\n",
    "        file_types = [\"csv\", \"parquet\"]\n",
    "\n",
    "    print(f\"ðŸ“‚ Procesando carga para capa: '{layer}'...\")\n",
    "\n",
    "    for filename in os.listdir(local_folder):\n",
    "\n",
    "        local_path = os.path.join(local_folder, filename)\n",
    "\n",
    "        # ValidaciÃ³n: que sea archivo y que tenga extensiÃ³n vÃ¡lida\n",
    "        if not os.path.isfile(local_path):\n",
    "            continue\n",
    "            \n",
    "        ext = filename.split(\".\")[-1].lower()\n",
    "        \n",
    "        if ext in file_types:\n",
    "            # Nombre del dataset = filename sin extensiÃ³n\n",
    "            dataset_folder = filename.split(\".\")[0]\n",
    "\n",
    "            # S3 path: raw/customers/customers.csv\n",
    "            s3_key = f\"{layer}/{dataset_folder}/{filename}\"\n",
    "\n",
    "            # Tags\n",
    "            tags = (\n",
    "                f\"layer={layer}&owner=Ezequiel Coggiola&team=Datera\"\n",
    "                f\"&env=dev&dataset={dataset_name}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                s3.upload_file(\n",
    "                    local_path,\n",
    "                    bucket_name,\n",
    "                    s3_key,\n",
    "                    ExtraArgs={\"Tagging\": tags}\n",
    "                )\n",
    "                print(f\"   âœ… Subido: {filename} -> {s3_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error subiendo {filename}: {str(e)}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# upload_files(\"./mis_datos\", bucket_name, layer=\"raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aadb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Procesando carga para capa: 'raw'...\n",
      "   âœ… Subido: customers.csv -> raw/customers/customers.csv\n",
      "   âœ… Subido: events.csv -> raw/events/events.csv\n",
      "   âœ… Subido: orders.csv -> raw/orders/orders.csv\n",
      "   âœ… Subido: order_items.csv -> raw/order_items/order_items.csv\n",
      "   âœ… Subido: products.csv -> raw/products/products.csv\n",
      "   âœ… Subido: reviews.csv -> raw/reviews/reviews.csv\n",
      "   âœ… Subido: sessions.csv -> raw/sessions/sessions.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Ejecutar\n",
    "\n",
    "local_folder = \"../dataset-ecommerce\"\n",
    "\n",
    "upload_files(local_folder, bucket_name, layer=\"raw\", file_types=\"csv\")\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=\"raw/2025/\")\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571bf3c",
   "metadata": {},
   "source": [
    "**Correr en terminal para ejecutar .sql en RDS:**\n",
    "\n",
    "    -- password: benji1234\n",
    "\n",
    "    -- Me conecto a la instancia, y ejecuto el archivo\n",
    "\n",
    "    mysql --local-infile=1 -h \"ENDPOINT\" -u admin -p < /home/ezecoggiola/proyectos/rds/load_files_rds.sql\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
